{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Markov Decision Process\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**You need to complete the TODO list below, and show the results in a screenshot on Crowdmark.**\n",
        "\n",
        "\n",
        "---\n",
        "Define a set of states, actions, transitions, and rewards\n"
      ],
      "metadata": {
        "id": "StxwOYNVYbgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MDP\n",
        "states = [\"Sleepy\", \"Hungry\", \"Studying\"]\n",
        "actions = {\"Sleepy\": [\"Sleep\"], \"Hungry\": [\"Eat\"], \"Studying\": [\"Study\"]}\n",
        "\n",
        "transitions = {\n",
        "    (\"Sleepy\", \"Sleep\"): [(\"Sleepy\", 0.8), (\"Hungry\", 0.2)],\n",
        "    (\"Hungry\", \"Eat\"): [(\"Hungry\", 0.9), (\"Studying\", 0.1)],\n",
        "    (\"Studying\", \"Study\"): [(\"Studying\", 0.7), (\"Sleepy\", 0.3)],\n",
        "}\n",
        "\n",
        "rewards = {\n",
        "    (\"Sleepy\", \"Sleep\", \"Sleepy\"): 10,\n",
        "    (\"Sleepy\", \"Sleep\", \"Hungry\"): 10,\n",
        "    (\"Hungry\", \"Eat\", \"Hungry\"): 5,\n",
        "    (\"Hungry\", \"Eat\", \"Studying\"): 5,\n",
        "    (\"Studying\", \"Study\", \"Studying\"): 15,\n",
        "    (\"Studying\", \"Study\", \"Sleepy\"): 15,\n",
        "}\n",
        "\n",
        "gamma = 0.9\n",
        "theta = 0.01  # Convergence threshold\n"
      ],
      "metadata": {
        "id": "GNiU1VhUYbtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Complete the code for value iteration.\n",
        "\n",
        "TODO: Inside the function, print how values change for each state in every iteration.\n",
        "\n",
        "TODO: Your return value should contain all optimal values for each state."
      ],
      "metadata": {
        "id": "zQHSWAOIYudA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(states, actions, transitions, rewards, gamma, theta):\n",
        "    # Complete your code here\n",
        "    #return the value"
      ],
      "metadata": {
        "id": "AN7i_CTVYxEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final optimal value output"
      ],
      "metadata": {
        "id": "UfaHSIMbbFyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_values = value_iteration(states, actions, transitions, rewards, gamma, theta)\n",
        "\n",
        "print(\"\\nFinal Optimal Values:\")\n",
        "for state in optimal_values:\n",
        "    print(f\"{state}: {optimal_values[state]:.2f}\")\n"
      ],
      "metadata": {
        "id": "ILI-r-AZYygN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}